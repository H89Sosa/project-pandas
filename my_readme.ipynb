{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* REUSABILIDAD. Usar funciones para la limpieza de datos. **EXPLICARLAS**\n",
    "* En fecha\n",
    "* Que funcione: run all\n",
    "* README con instrucciones de lo que han hecho  (plantilla)\n",
    "* Comentarios en el código\n",
    "* Código erróneo o que no sirve no ponerlo en la versión definitiva\n",
    "* Archivos en carpetas /data , /images, /old_version\n",
    "* Las librerías nuevas que importen arriba para poder instalarlas y verlas\n",
    "\n",
    "\n",
    "### Funcion para quitar columnas si NA's > 80%\n",
    "### Geolocalización?\n",
    "# UNIFICAR INDICE DE README Y DEL CODE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![IronHack Logo](https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/upload_d5c5793015fec3be28a63c4fa3dd4d55.png)\n",
    "\n",
    "# *PANDAS CLEANING PROJECT*\n",
    "## by Hernán Sosa.\n",
    "\n",
    "We are given a very untidy and messy dataset from Kaggle Datasets [Sharks Dataset](https://www.kaggle.com/teajay/global-shark-attacks/version/1). This dataset contains records on shark attacks over history. \n",
    "Our goal is to provide a consistent Dataset to properly show up **'Shark attack cases over time and place'**. In order to get useful information, we processed this dataset by following the next steps:\n",
    "\n",
    "## 0. Setting up Workspace:\n",
    "\n",
    "Since we've done some different processes along the project, we've also used different libraries:\n",
    "\n",
    "- `import re`: Imports the *Regex* library. Useful for text manipulation.\n",
    "\n",
    "- `import pandas as pd`: *Pandas* library. The core of our DataFrame structure.\n",
    "\n",
    "- `import numpy as np`: *Numpy* lets us manipulate arrays, numeric operations and brings many other useful functions.\n",
    "\n",
    "- `import datetime`: *Datetime* is needed in order to get a better date manipulation.\n",
    "\n",
    "- `import random`: *Random* is used to generate random values in different approaches.\n",
    "\n",
    "- `import seaborn as sns`: *Seaborn* is a library to generate prettier graphics.\n",
    "\n",
    "- `import matplotlib.pyplot as plt`: *Matplotlib*, used for graphic creation. It's integration with *Pandas* makes it very useful.\n",
    "\n",
    "- `%matplotlib inline`: Jupyter and *Matplotlib* need this code in order to get proper visualizations on the notebook.\n",
    "\n",
    "- `from pandas.plotting import register_matplotlib_converters`: Future versions of *Pandas* wiil need this library in order to show plots correctly.\n",
    "\n",
    "After that, we're importing our ``'sharks.csv'`` file and transform it to a DataFrame through ``pd.read_csv()``\n",
    "\n",
    "\n",
    "## 1. Analyzing Dataset :\n",
    "\n",
    "As we said, our Dataset is untidy. It contains the following columns:\n",
    "\n",
    "           'Case Number', 'Date', 'Year', 'Type', 'Country', 'Area', 'Location',\n",
    "           'Activity', 'Name', 'Sex ', 'Age', 'Injury', 'Fatal (Y/N)', 'Time',\n",
    "           'Species ', 'Investigator or Source', 'pdf', 'href formula', 'href',\n",
    "           'Case Number.1', 'Case Number.2', 'original order', 'Unnamed: 22',\n",
    "           'Unnamed: 23'\n",
    "\n",
    "\n",
    "Using our custom function `df_total_na(df)` we can see that our dataset has a total **19.15%** missing values overall.\n",
    "\n",
    "Also, our function `column_nulls_percentage(df)` shows total percentage of missing values *per row*. Showing the ones with `Nan` over 40%:\n",
    "\n",
    "            Age                       44.74\n",
    "            Time                      53.62\n",
    "            Species                   48.97\n",
    "            Unnamed: 22               99.98\n",
    "            Unnamed: 23               99.97\n",
    "        \n",
    "### 1.1 Erasing unuseful columns:\n",
    "\n",
    "After a quick exploration using `df.info()`, `df.value_counts`, and `df.head()` we quickly realized that we don't need the following columns, so we're taking them out with `df.drop()`:\n",
    "\n",
    "- `'Case Number'`, `'Case Number.1'`, `'Case Number.2'` provides IDs not useful for us, since we plan to have a DateTime index, and contains too many NA's.\n",
    "\n",
    "- `'pdf'` contains unuseful name archives.\n",
    "\n",
    "- `'Year'` contains duplicated data, also present in `'Dates'`, and contains 44.7% null values.\n",
    "\n",
    "- `'Type'` contains unclear values for our study.\n",
    "\n",
    "- `'href formula'`, `'href'` contains links leading to non-existing web pages.\n",
    "\n",
    "Then, we proceed to clean the column names and `.lower()`them down, just for practical pruposes.\n",
    "\n",
    "This is what's left:\n",
    "\n",
    "        Index(['date', 'country', 'area', 'location', 'activity', 'name', 'sex', 'age',\n",
    "        'injury', 'fatal', 'time', 'species ', 'source'])\n",
    "\n",
    "\n",
    "## 2. Cleaning Dataset:\n",
    "\n",
    "We'll treat columns one by one, since all of them should behave in a particular way.\n",
    "\n",
    "- ###  2.1 Setting up a DateTime Index:\n",
    "\n",
    "\n",
    "We can set the index of the DataFrame in order to have a proper way to study it, using the `'date'` column values:\n",
    "\n",
    "            0    18-Sep-16\n",
    "            1    18-Sep-16\n",
    "            2    18-Sep-16\n",
    "            3    17-Sep-16\n",
    "            4    16-Sep-16\n",
    "            5    15-Sep-16\n",
    "        \n",
    "This format is good for us, but not the `dtype`. So we'll just set all the column values to datetime objects, and set the non-readable values to `NaT`, using`pd.to_datetime()`.\n",
    "\n",
    "This process sacrifices 857 values, **15% of our observations**. Analyzing the points, in the original `Series` we conclude that these values were not useful due its ambiguity.\n",
    "\n",
    "Finally, we set the `df.index` to a this new DateTime series:\n",
    "\n",
    "            df = df.set_index(df_time)\n",
    "        \n",
    "Additionally, with `df.index.rename('')` and `df.index.strftime('%B %d, %Y')` we set proper ways to visualize this data.\n",
    "\n",
    "\n",
    "- ### 2.2 Location manipulation\n",
    "\n",
    "\n",
    "On the format we've given, we can see 3 rows which can be converted to just one by joining them and manipulating its content: `'country'`, `'area'` and `'location'`.\n",
    "\n",
    "We'll transform all the values of these columns with `capitalize()`, a custom function that takes every word of a string and `str.Title` it. Once we have this processed strings, we join them on a single one using the `zip()` function to concatenate rows in order.\n",
    "\n",
    "With this values, we'll be able to tranform them into 'latitude' and 'altutide' coordinates to geolocalize them (coming soon)\n",
    "\n",
    "- ### 2.3 'Age' column\n",
    "\n",
    "\n",
    "We'll use Regex and `str.contains` to see how much of the values in 'age' have valid data points. We see that we have a total 3280 '1 or 2 digit' type observations:\n",
    "\n",
    "            True     3280\n",
    "            NaN      2681\n",
    "            False      31\n",
    "            \n",
    "The age of the victims is really not important in our case study, but it's information that might be handy in some cases.\n",
    "So, we'll process all the data points with `clean_2d_age(string)`, another custom function that takes a string containing numbers and converts it to a one or two digits integer using RegEx. Now we have a proper `dtype=int` column for this values.\n",
    "\n",
    "- ### 2.4 'Fatal' column: Bool variables\n",
    "\n",
    "In this column we should expect to have just two values, `yes` and `no`:\n",
    "\n",
    "            N          4315\n",
    "            Y          1552\n",
    "            UNKNOWN      94\n",
    "            nan          19\n",
    "             N            8\n",
    "            n             1\n",
    "            F             1\n",
    "            #VALUE!       1\n",
    "            N             1\n",
    "    \n",
    "Not the expected output. For a better approach, we could use `True` and `False` so we'll use the custom function (using RegEx) `yn_to_tf(string)`.\n",
    "This function analyzes a string using regex. If the string is Yes, return True. If it is No, return False. For invalid data, returns Nan.\n",
    "\n",
    "            False    4440\n",
    "            True     1552\n",
    "            \n",
    "- ### 2.5 'Sex' column: Categorical variables\n",
    "\n",
    "Similarly to 'Fatal' column, `sex` should have only 3 observations: `male`, `female`, and `nan` (since Unknown it's not a valid value for our study)\n",
    "\n",
    "We're making use of the function `mf_to_malefemale(string)`, which is a variation of the one that we used before.\n",
    "\n",
    "After assigning the processed values to this column, we end up with 4837 `male` and 585 `female` values.\n",
    "        \n",
    "\n",
    "- ### 2.6 'Name' column: Assigning unwanted variables to Nan\n",
    "\n",
    "The 'name' column it's a pretty messy one. We can't afford to process this values, since "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
